
## L16 Thinking
**1.机器学习中的监督学习、非监督学习、强化学习有何区别**    
监督学习就是有标签，有y目标的学习，非监督学习就是无标签的学习，比如聚类算法。强化学习，就是一种在实践中反复学习。先定义state,然后学习的agent可以采取哪些action,每一种action对应的reward，学习的目标累计的reward最大。     
 

**2.什么是策略网络，价值网络，有何区别**            
Policy-base,直接学习策略本身，获得策略函数。对于给定的输入，通过学习给出一个确定输出的网络：（动作1，状态1），（动作2，状态2）
 
Value-base，通过学习价值函数指导策略制定。计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络，奖励更多的状态，会在数值网络中的数值Value更大。
     


**3.请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的**     
在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树。    
选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点       
扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点     
模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果        
回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上 


**4.假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑**              
首先要定义状态(state)、动作(action)、策略（policy）、奖励(reward)。比如定义状态，可以包括当天的日期，什么季节，工作日还是休息日，是否接近某个节日，用户的属性信息也可以定义为状态，比如用户的年龄性别地域，该用户的浏览历史等等。定义动作，该用户浏览哪一类视频，是否点赞收藏喜欢，该用户是否自己发布视频。定义策略，如何推荐，推荐和用户基本属性最接近的其他用户浏览过的视频，还是推荐和用户上一个浏览点赞喜欢的的视频最类似的视频，还是推荐全站最火的视频。定义reward，用户是否点赞喜欢后续推荐的视频，是否评论互动。


**5.在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路**      
